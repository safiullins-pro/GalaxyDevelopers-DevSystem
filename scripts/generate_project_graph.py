#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≥—Ä–∞—Ñ–∞ –ø—Ä–æ–µ–∫—Ç–∞ –∏–∑ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ñ–∞–π–ª–æ–≤
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, –Ω–∞—Ö–æ–¥–∏—Ç –º—É—Å–æ—Ä, —Å–æ–∑–¥–∞–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
"""

import json
import os
from datetime import datetime, timedelta
from pathlib import Path
import hashlib

class ProjectAnalyzer:
    def __init__(self, base_path="/Volumes/Z7S/development/GalaxyDevelopers/DevSystem"):
        self.base_path = base_path
        self.files_structure_path = f"{base_path}/DOCUMENTS/FILES_STRUCTURE"
        self.project_data = {}
        self.dependencies = {}
        self.orphaned = []
        self.deprecated = []
        self.duplicates = []
        
    def load_all_json_structures(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ JSON —Ñ–∞–π–ª—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        for json_file in Path(self.files_structure_path).glob("*.json"):
            with open(json_file, 'r') as f:
                data = json.load(f)
                self.project_data[data.get('directory', json_file.stem)] = data
    
    def analyze_dependencies(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏"""
        for dir_name, dir_data in self.project_data.items():
            files = dir_data.get('files', {})
            for file_name, file_info in files.items():
                file_path = f"{dir_name}/{file_name}"
                
                # –°–æ–±–∏—Ä–∞–µ–º –∏–º–ø–æ—Ä—Ç—ã/—ç–∫—Å–ø–æ—Ä—Ç—ã
                connections = file_info.get('connections', [])
                imports = file_info.get('functionality', {}).get('dependencies', [])
                
                self.dependencies[file_path] = {
                    'imports': connections + imports,
                    'imported_by': [],
                    'tags': file_info.get('tags', [])
                }
        
        # –û–±—Ä–∞—Ç–Ω—ã–µ —Å–≤—è–∑–∏
        for file_path, deps in self.dependencies.items():
            for imported_file in deps['imports']:
                if imported_file in self.dependencies:
                    self.dependencies[imported_file]['imported_by'].append(file_path)
    
    def detect_garbage(self):
        """–ù–∞—Ö–æ–¥–∏–º –º—É—Å–æ—Ä –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
        
        for file_path, deps in self.dependencies.items():
            # 1. Orphaned - —Ñ–∞–π–ª—ã –±–µ–∑ —Å–≤—è–∑–µ–π
            if not deps['imports'] and not deps['imported_by']:
                if 'test' not in file_path.lower() and 'backup' not in file_path.lower():
                    self.orphaned.append({
                        'file': file_path,
                        'reason': '–ù–µ—Ç –∏–º–ø–æ—Ä—Ç–æ–≤ –∏ –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è',
                        'severity': 'high'
                    })
            
            # 2. Deprecated - —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ —Ñ–∞–π–ª—ã
            if 'deprecated' in deps.get('tags', []):
                self.deprecated.append({
                    'file': file_path,
                    'reason': '–ü–æ–º–µ—á–µ–Ω –∫–∞–∫ deprecated',
                    'severity': 'medium'
                })
            
            # 3. Test/Backup —Ñ–∞–π–ª—ã —Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π
            if 'test' in file_path.lower() or 'backup' in file_path.lower():
                file_full_path = f"{self.base_path}/{file_path}"
                if os.path.exists(file_full_path):
                    mtime = datetime.fromtimestamp(os.path.getmtime(file_full_path))
                    if datetime.now() - mtime > timedelta(days=30):
                        self.deprecated.append({
                            'file': file_path,
                            'reason': f'–¢–µ—Å—Ç–æ–≤—ã–π/backup —Ñ–∞–π–ª —Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π',
                            'severity': 'low'
                        })
    
    def find_duplicates(self):
        """–ò—â–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–π—Å—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª"""
        functionality_hashes = {}
        
        for dir_name, dir_data in self.project_data.items():
            files = dir_data.get('files', {})
            for file_name, file_info in files.items():
                # –°–æ–∑–¥–∞–µ–º —Ö–µ—à –∏–∑ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞
                functionality = json.dumps(file_info.get('functionality', {}), sort_keys=True)
                func_hash = hashlib.md5(functionality.encode()).hexdigest()[:8]
                
                if func_hash in functionality_hashes:
                    self.duplicates.append({
                        'files': [functionality_hashes[func_hash], f"{dir_name}/{file_name}"],
                        'reason': '–ü–æ—Ö–æ–∂–∏–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª',
                        'severity': 'medium'
                    })
                else:
                    functionality_hashes[func_hash] = f"{dir_name}/{file_name}"
    
    def generate_graph(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≥—Ä–∞—Ñ –ø—Ä–æ–µ–∫—Ç–∞"""
        output = []
        output.append("=" * 60)
        output.append("–°–¢–†–£–ö–¢–£–†–ê –ü–†–û–ï–ö–¢–ê GalaxyDevelopers")
        output.append(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        output.append("=" * 60)
        output.append("")
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
        for dir_name, dir_data in sorted(self.project_data.items()):
            tags = ', '.join(dir_data.get('tags', []))
            output.append(f"üìÅ {dir_name}/ [{tags}]")
            output.append(f"   {dir_data.get('description', '')}")
            
            # –§–∞–π–ª—ã
            files = dir_data.get('files', {})
            for file_name, file_info in sorted(files.items()):
                file_tags = ', '.join(file_info.get('tags', []))
                output.append(f"   üìÑ {file_name} [{file_tags}]")
                output.append(f"      {file_info.get('description', '')}")
                
                # –°–≤—è–∑–∏
                connections = file_info.get('connections', [])
                if connections:
                    output.append(f"      ‚Üí –°–≤—è–∑–∏: {', '.join(connections)}")
            
            output.append("")
        
        # –ê–Ω–∞–ª–∏–∑ –º—É—Å–æ—Ä–∞
        output.append("=" * 60)
        output.append("–ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –ö–û–î–ê")
        output.append("=" * 60)
        output.append("")
        
        if self.orphaned:
            output.append("üî¥ ORPHANED FILES (—Ñ–∞–π–ª—ã –±–µ–∑ —Å–≤—è–∑–µ–π):")
            for item in self.orphaned:
                output.append(f"   - {item['file']}: {item['reason']}")
            output.append("")
        
        if self.deprecated:
            output.append("üü° DEPRECATED/OLD FILES:")
            for item in self.deprecated:
                output.append(f"   - {item['file']}: {item['reason']}")
            output.append("")
        
        if self.duplicates:
            output.append("üü† –í–û–ó–ú–û–ñ–ù–´–ï –î–£–ë–õ–ò–ö–ê–¢–´:")
            for item in self.duplicates:
                output.append(f"   - {', '.join(item['files'])}: {item['reason']}")
            output.append("")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        output.append("=" * 60)
        output.append("–°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        output.append("=" * 60)
        total_files = sum(len(d.get('files', {})) for d in self.project_data.values())
        output.append(f"–í—Å–µ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π: {len(self.project_data)}")
        output.append(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
        output.append(f"–§–∞–π–ª–æ–≤ –±–µ–∑ —Å–≤—è–∑–µ–π: {len(self.orphaned)}")
        output.append(f"–£—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤: {len(self.deprecated)}")
        output.append(f"–í–æ–∑–º–æ–∂–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(self.duplicates)}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        garbage_count = len(self.orphaned) + len(self.deprecated)
        if garbage_count > 0:
            output.append("")
            output.append("‚ö†Ô∏è  –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
            output.append(f"   –ú–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å {garbage_count} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞")
            if self.orphaned:
                output.append(f"   –ü—Ä–æ–≤–µ—Ä–∏—Ç—å {len(self.orphaned)} —Ñ–∞–π–ª–æ–≤ –±–µ–∑ —Å–≤—è–∑–µ–π")
        
        return "\n".join(output)
    
    def generate_context_for_ai(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI"""
        context = {
            "project": "GalaxyDevelopers DevSystem",
            "structure": {},
            "critical_files": [],
            "api_endpoints": [],
            "ports": {"backend": 37777, "memory": 37778},
            "issues": []
        }
        
        for dir_name, dir_data in self.project_data.items():
            context["structure"][dir_name] = {
                "purpose": dir_data.get("description", ""),
                "files_count": len(dir_data.get("files", {})),
                "key_files": list(dir_data.get("files", {}).keys())[:3]
            }
            
            # –°–æ–±–∏—Ä–∞–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
            for file_name, file_info in dir_data.get("files", {}).items():
                if "critical" in file_info.get("tags", []):
                    context["critical_files"].append(f"{dir_name}/{file_name}")
                
                # API endpoints
                if "api_endpoints" in file_info.get("functionality", {}):
                    context["api_endpoints"].extend(
                        file_info["functionality"]["api_endpoints"]
                    )
        
        # –ü—Ä–æ–±–ª–µ–º—ã
        if self.orphaned:
            context["issues"].append(f"{len(self.orphaned)} orphaned files")
        if self.deprecated:
            context["issues"].append(f"{len(self.deprecated)} deprecated files")
        
        return json.dumps(context, indent=2, ensure_ascii=False)
    
    def run(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        print("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞...")
        self.load_all_json_structures()
        
        print("üîó –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏...")
        self.analyze_dependencies()
        
        print("üóëÔ∏è  –ò—â–µ–º –º—É—Å–æ—Ä...")
        self.detect_garbage()
        self.find_duplicates()
        
        print("üìä –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≥—Ä–∞—Ñ...")
        graph = self.generate_graph()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        output_path = f"{self.base_path}/DOCUMENTS/project_graph.txt"
        with open(output_path, 'w') as f:
            f.write(graph)
        print(f"‚úÖ –ì—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_path}")
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI
        context_path = f"{self.base_path}/DOCUMENTS/ai_context.json"
        with open(context_path, 'w') as f:
            f.write(self.generate_context_for_ai())
        print(f"‚úÖ AI –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {context_path}")
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        print("\n" + graph)
        
        return {
            "orphaned": len(self.orphaned),
            "deprecated": len(self.deprecated),
            "duplicates": len(self.duplicates),
            "total_garbage": len(self.orphaned) + len(self.deprecated)
        }

if __name__ == "__main__":
    analyzer = ProjectAnalyzer()
    results = analyzer.run()
    
    if results["total_garbage"] > 0:
        print(f"\nüö® –ù–∞–π–¥–µ–Ω–æ {results['total_garbage']} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏!")
        print("–ó–∞–ø—É—Å—Ç–∏—Ç–µ 'python3 clean_garbage.py' –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")